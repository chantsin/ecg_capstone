{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13dc42e2",
   "metadata": {},
   "source": [
    "## ECG Classification Using Convolutional Neural Networks\n",
    "\n",
    "Author: Calvin Chan \n",
    "\n",
    "### Introduction\n",
    "In our last notebook, we will use the concept of convolution to model our ECG timeseries data. Traditionally, image analysis uses a 2D convolution to extract features from images, however since we are working with signals this would not work. Instead of 2D convolutions, we can convolve over our signals using a 1D framework. \n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "Here, we import all the necessary packages used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "db2e2f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import load_functions as f\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_curve, roc_auc_score\n",
    "from tqdm.notebook import trange, tnrange, tqdm_notebook\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from Notebooks import ecg_cleaning as c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2385607",
   "metadata": {},
   "source": [
    "### Data Import\n",
    "\n",
    "Below we load our full data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e6ffd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Path\n",
    "path = '../data/physionet.org/files/ptb-xl/1.0.3/'\n",
    "metadata = pd.read_csv('../data/cleaned_metadata.csv')\n",
    "\n",
    "# Import data\n",
    "full_data = f.load_signal(path, metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f77e9b31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15677, 1000, 12), (15677,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at the data shape \n",
    "full_data[0].shape, full_data[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbb5e0b",
   "metadata": {},
   "source": [
    "We also want to look at Lead II only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34590d4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15677, 1000), (15677,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Slicing only for Lead II \n",
    "X = full_data[0][:,:,1]\n",
    "y = full_data[1]\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408f7b31",
   "metadata": {},
   "source": [
    "### Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a967adc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform target column into binary classes\n",
    "y = y.apply(lambda x: f.binary(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36b893bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "diagnostic_superclass\n",
       "ABNO    8645\n",
       "NORM    7032\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7b53dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate LabelEncoder \n",
    "label = LabelEncoder()\n",
    "\n",
    "# Fit binary classes\n",
    "label.fit(y)\n",
    "\n",
    "# Transform classes\n",
    "y = label.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cb26b72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking how our label is encoded\n",
    "label.transform(['ABNO', 'NORM'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ffe05f",
   "metadata": {},
   "source": [
    "As we can see, signals that are labeled `ABNO` are mapped to `0` and `NORM` are mapped `1`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29d05a1",
   "metadata": {},
   "source": [
    "#### Signal denoising using ecg_cleaning.py\n",
    "\n",
    "In this following section, we denoise our ECG signals using Fourier Tranforms. We have to specify the frequencies in which we want to take out for baseline wandering and powerline interference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3239935f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15677, 1000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20ee70d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low sampling frequency\n",
    "sig_len = 1000\n",
    "sampling_frequency = 100\n",
    "time = np.arange(0, sig_len) / sampling_frequency\n",
    "\n",
    "# Baseline and PLI removal\n",
    "signal_bl = np.apply_along_axis(c.baseline_removal, axis=1, arr=X, freq_start=0.1, freq_stop=1.5)\n",
    "signal_pli = np.apply_along_axis(c.high_freq_removal, axis=1, arr=signal_bl, freq_start=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9160008c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15677, 1000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal_pli.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9149508",
   "metadata": {},
   "source": [
    "#### Train Test Split\n",
    "\n",
    "After preprocessing our signals, we can split our data set to training and testing. We will use `20%` of our data as testing and the rest as training. Since we have a data imbalance, we will include `stratify` as well.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d6ca143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12541, 1000), (3136, 1000), (12541,), (3136,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train/Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(signal_pli, y, test_size=0.2, stratify=y)\n",
    "\n",
    "# Checking the shape\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcb0132",
   "metadata": {},
   "source": [
    "#### Scaling \n",
    "\n",
    "Next, we want to scale our data. Since we are using using a neural network, as we calculate the gradient during backpropagation, distances can affect how well this is performed. We will use `StandardScaler` for our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "877e1a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate standard scaler\n",
    "ss = StandardScaler()\n",
    "\n",
    "# Fit and transform training set\n",
    "X_train = ss.fit_transform(X_train)\n",
    "\n",
    "# Transform testing set\n",
    "X_test = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea219c4",
   "metadata": {},
   "source": [
    "#### Transforming arrays into Torch Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce48c226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12541, 1000]) torch.Size([3136, 1000]) torch.Size([12541]) torch.Size([3136])\n"
     ]
    }
   ],
   "source": [
    "# Transforming independent variables\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "# Transforming dependent variables\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Sanity check\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f42f4a",
   "metadata": {},
   "source": [
    "#### Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d498b257",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Setting batch size #######\n",
    "batch_size = 128\n",
    "\n",
    "# Shuffle training set\n",
    "training_loader = DataLoader(TensorDataset(X_train.unsqueeze(1), y_train.unsqueeze(1)), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Don't shuffle validation set\n",
    "testing_loader = DataLoader(TensorDataset(X_test.unsqueeze(1), y_test.unsqueeze(1)), batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7569a59",
   "metadata": {},
   "source": [
    "### Model Setup and Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "53f4cbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    \"\"\"Starting off with a simple cnn.\"\"\"\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        # Defining convolution layers\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            \n",
    "            # Convolution block 1\n",
    "            nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=None),\n",
    "            \n",
    "            # Convolution block 2\n",
    "            nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=None),\n",
    "        )\n",
    "        \n",
    "        # Drop out layer \n",
    "        self.dropout = nn.Dropout1d(0.15)\n",
    "        \n",
    "        # Fully-connected layer\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(in_features=32*250, out_features=400),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_features=400, out_features=16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_features=16, out_features=1),\n",
    "        )\n",
    "        \n",
    "        # Output layer \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # Loss function\n",
    "        self.binary_entropy_loss = nn.BCELoss()\n",
    "        self.optimizer = torch.optim.SGD(self.parameters(), lr=0.01)\n",
    "        \n",
    "    def forward(self, x): \n",
    "        \"\"\"Perform forward pass.\"\"\"\n",
    "\n",
    "        # Pass through convolution layers\n",
    "        x = self.conv_layer(x)\n",
    "        \n",
    "        # Flatten the output after convolution\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Apply drop out\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Pass through fully-connected layer\n",
    "        x = self.fc_layer(x)\n",
    "\n",
    "        # If training, repeat, else, compute output layer\n",
    "        if not self.training:\n",
    "            x = self.sigmoid(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"Make prediction.\"\"\"\n",
    "\n",
    "        predictions = self.forward(x)\n",
    "\n",
    "        # Hard class prediction: output from sigmoid with the higher percentage\n",
    "        hard_class_prediction = torch.argmax(predictions, dim=1)\n",
    "\n",
    "        return hard_class_prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dbe397",
   "metadata": {},
   "source": [
    "#### Example training of 1 signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "73a26591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000]) torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "# Sample training \n",
    "\n",
    "sample = X_train[0]\n",
    "sample_label = y_train[0]\n",
    "\n",
    "print(sample.shape, sample_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c55d60ad",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1000])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.unsqueeze(0).unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6232a4ba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleCNN(\n",
       "  (conv_layer): Sequential(\n",
       "    (0): Conv1d(1, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv1d(16, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (dropout): Dropout1d(p=0.15, inplace=False)\n",
       "  (fc_layer): Sequential(\n",
       "    (0): Linear(in_features=8000, out_features=400, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=400, out_features=16, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Linear(in_features=16, out_features=1, bias=True)\n",
       "  )\n",
       "  (sigmoid): Sigmoid()\n",
       "  (binary_entropy_loss): BCELoss()\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing the model\n",
    "\n",
    "cnn_model = SimpleCNN()\n",
    "cnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "747da1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear any gradients from previous run (in this case this is our first and only run)\n",
    "cnn_model.optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1b2740ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass through 1 training signal to model after reshaping size\n",
    "outputs = cnn_model(sample.unsqueeze(0).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ef01e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the binary entropy loss after one training and label \n",
    "loss = cnn_model.binary_entropy_loss(outputs.sigmoid(), sample_label.unsqueeze(0).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d828989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7757506370544434"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loss\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d0f470b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagation \n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b87b04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update weights\n",
    "cnn_model.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8fbeb6",
   "metadata": {},
   "source": [
    "### Simple Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e83c822e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "\n",
    "CNN_model_2 = SimpleCNN()\n",
    "\n",
    "binary_entropy_loss = nn.BCELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(CNN_model_2.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a5474811",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ae511d5a6a84dc7af6d4a14ed3c2054",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Total epochs ::   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1\n",
      "LOSS train 61.419566571712494 valid 0.7053996324539185\n",
      "EPOCH 2\n",
      "LOSS train 57.10231393575668 valid 0.7118154168128967\n",
      "EPOCH 3\n",
      "LOSS train 53.25187286734581 valid 0.6865294575691223\n",
      "EPOCH 4\n",
      "LOSS train 48.22863185405731 valid 0.6868211627006531\n",
      "EPOCH 5\n",
      "LOSS train 43.50026321411133 valid 0.664866030216217\n",
      "EPOCH 6\n",
      "LOSS train 40.67124545574188 valid 0.6489605903625488\n",
      "EPOCH 7\n",
      "LOSS train 35.24500493705273 valid 0.6426914930343628\n",
      "EPOCH 8\n",
      "LOSS train 29.749459877610207 valid 0.6524726152420044\n",
      "EPOCH 9\n",
      "LOSS train 23.4884831905365 valid 0.6363000273704529\n",
      "EPOCH 10\n",
      "LOSS train 19.365352980792522 valid 0.633962869644165\n",
      "EPOCH 11\n",
      "LOSS train 17.396748900413513 valid 0.6377379894256592\n",
      "EPOCH 12\n",
      "LOSS train 13.715776715427637 valid 0.6354734897613525\n",
      "EPOCH 13\n",
      "LOSS train 14.63328082114458 valid 0.6501156687736511\n",
      "EPOCH 14\n",
      "LOSS train 14.072751995176077 valid 0.6410595178604126\n",
      "EPOCH 15\n",
      "LOSS train 12.660690296441317 valid 0.6398739814758301\n",
      "EPOCH 16\n",
      "LOSS train 13.73697393015027 valid 0.6412565112113953\n",
      "EPOCH 17\n",
      "LOSS train 12.662517577409744 valid 0.6393258571624756\n",
      "EPOCH 18\n",
      "LOSS train 13.479348380118608 valid 0.6376328468322754\n",
      "EPOCH 19\n",
      "LOSS train 11.957148522138596 valid 0.6369568705558777\n",
      "EPOCH 20\n",
      "LOSS train 12.337939646095037 valid 0.6374585628509521\n",
      "EPOCH 21\n",
      "LOSS train 12.962305091321468 valid 0.6370251774787903\n",
      "EPOCH 22\n",
      "LOSS train 13.443849224597216 valid 0.6370804905891418\n",
      "EPOCH 23\n",
      "LOSS train 12.504390113055706 valid 0.6374028921127319\n",
      "EPOCH 24\n",
      "LOSS train 12.327265039086342 valid 0.6368692517280579\n",
      "EPOCH 25\n",
      "LOSS train 13.16043421253562 valid 0.6368240714073181\n",
      "Optimization ended successfully\n"
     ]
    }
   ],
   "source": [
    "epoch_number = 0\n",
    "\n",
    "total_train_loss = []\n",
    "total_test_loss = []\n",
    "\n",
    "for epoch in trange(25, desc=\"Total epochs :\"):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "    \n",
    "    # Training \n",
    "    running_loss = 0\n",
    "    last_loss = 0\n",
    "    \n",
    "    CNN_model_2.train(True)\n",
    "    \n",
    "    for i, data in enumerate(training_loader):\n",
    "        \n",
    "        inputs, labels = data\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = CNN_model_2(inputs)\n",
    "        \n",
    "        loss = binary_entropy_loss(outputs.sigmoid(), labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    total_train_loss.append(running_loss)\n",
    "    \n",
    "    # Testing\n",
    "    running_tloss = 0.0\n",
    "    \n",
    "    CNN_model_2.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, tdata in enumerate(testing_loader):\n",
    "            tinputs, tlabels = tdata\n",
    "            toutputs = CNN_model_2(tinputs)\n",
    "            tloss = binary_entropy_loss(toutputs.sigmoid(), tlabels)\n",
    "            running_tloss += tloss\n",
    "            \n",
    "    avg_tloss = running_tloss / (i + 1)\n",
    "    total_test_loss.append(avg_tloss)\n",
    "    \n",
    "    epoch_number += 1\n",
    "    \n",
    "    print('LOSS train {} test {}'.format(running_loss, avg_tloss))\n",
    "    \n",
    "print(f\"Optimization ended successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d3cecb",
   "metadata": {},
   "source": [
    "### Accuracy Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d3f6f144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "581f8608",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (32x250 and 8000x400)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mCNN_model_2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[45], line 63\u001b[0m, in \u001b[0;36mSimpleCNN.predict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     61\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Make prediction.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# Hard class prediction: output from sigmoid with the higher percentage\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     hard_class_prediction \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(predictions, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[45], line 52\u001b[0m, in \u001b[0;36mSimpleCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     49\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Pass through fully-connected layer\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# If training, repeat, else, compute output layer\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ecgcap/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ecgcap/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ecgcap/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ecgcap/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ecgcap/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ecgcap/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x250 and 8000x400)"
     ]
    }
   ],
   "source": [
    "CNN_model_2.predict(X_train[0].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c72f64fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score on train set: 55.15%\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "binary_classification = CNN_model_2.predict(X_train.unsqueeze(1))\n",
    "\n",
    "# Calculate the score on the test set\n",
    "accuracy = accuracy_score(y_train, binary_classification)\n",
    "print(f\"Accuracy score on train set: {round(accuracy * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7331ab34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score on train set: 55.13%\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "binary_classification = CNN_model_2.predict(X_test.unsqueeze(1))\n",
    "\n",
    "# Calculate the score on the test set\n",
    "accuracy = accuracy_score(y_test, binary_classification)\n",
    "print(f\"Accuracy score on testing set: {round(accuracy * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d34860b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.figure()\n",
    "plt.plot(total_train_loss, label=\"Training Loss\")\n",
    "plt.plot(total_test_loss, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759fdb30",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "7b6ffdf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12541, 1000])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "27341541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12541, 1])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " y_train.unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "818edc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For both sets\n",
    "train_data = TensorDataset(X_train.unsqueeze(1), y_train.unsqueeze(1))\n",
    "test_data = TensorDataset(X_test.unsqueeze(1), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "72093023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This iterator returns four training examples at a time so we will update the model after every 4 images\n",
    "train_dataloader = DataLoader(train_data, batch_size=8, shuffle=True)\n",
    "\n",
    "# This iterator returns 1024 test examples at a time (for fast testing)\n",
    "test_dataloader = DataLoader(test_data, batch_size=512, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a64b496d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initiliaze the model\n",
    "CNN_model = SimpleCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e64c1bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre training accuracy: 55.13392857142857%\n"
     ]
    }
   ],
   "source": [
    "def get_accuracy(model, dataloader):\n",
    "    '''\n",
    "    Helper function to get classification accuracy for a model over the items in dataloader.\n",
    "    taken from: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "    '''\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "\n",
    "    # Go through all of the data\n",
    "    for i, data in enumerate(test_dataloader, 0):\n",
    "    #for batch in dataloader:\n",
    "        images, labels = data\n",
    "\n",
    "        # Get the prediction of the net on the images\n",
    "        predicted = model.predict(images)\n",
    "\n",
    "\n",
    "        total += labels.size(0)\n",
    "\n",
    "        # Count those we got correct\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return(100 * correct / total)\n",
    "\n",
    "\n",
    "print(f\"Pre training accuracy: {get_accuracy(CNN_model, test_dataloader)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "bcab1794",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5c1c8a1379a41e7ba48af0ecf7d0e0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Total epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.6888 | Avg training accuracy: 441.07 | Avg validation accuracy: 55.13\n",
      "Avg loss: 0.6472 | Avg training accuracy: 441.07 | Avg validation accuracy: 55.13\n",
      "Avg loss: 0.5765 | Avg training accuracy: 441.07 | Avg validation accuracy: 55.13\n",
      "Avg loss: 0.532 | Avg training accuracy: 441.07 | Avg validation accuracy: 55.13\n",
      "Avg loss: 0.4616 | Avg training accuracy: 441.07 | Avg validation accuracy: 55.13\n",
      "Avg loss: 0.3551 | Avg training accuracy: 441.07 | Avg validation accuracy: 55.13\n",
      "Avg loss: 0.2617 | Avg training accuracy: 441.07 | Avg validation accuracy: 55.13\n",
      "Avg loss: 0.1991 | Avg training accuracy: 441.07 | Avg validation accuracy: 55.13\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[98], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m     loss_sum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# Get the prediction of the net on the images\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     predicted \u001b[38;5;241m=\u001b[39m \u001b[43mCNN_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     acc_sum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (labels \u001b[38;5;241m==\u001b[39m predicted)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m/\u001b[39m labels\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Print summary of training metrics\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 63\u001b[0m, in \u001b[0;36mSimpleCNN.predict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     61\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Make prediction.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# Hard class prediction: output from sigmoid with the higher percentage\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     hard_class_prediction \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(predictions, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[15], line 43\u001b[0m, in \u001b[0;36mSimpleCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Perform forward pass.\"\"\"\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Pass through convolution layers\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Flatten the output after convolution\u001b[39;00m\n\u001b[1;32m     46\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ecgcap/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ecgcap/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ecgcap/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ecgcap/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ecgcap/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ecgcap/lib/python3.10/site-packages/torch/nn/modules/pooling.py:91\u001b[0m, in \u001b[0;36mMaxPool1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor):\n\u001b[0;32m---> 91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_indices\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ecgcap/lib/python3.10/site-packages/torch/_jit_internal.py:497\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ecgcap/lib/python3.10/site-packages/torch/nn/functional.py:710\u001b[0m, in \u001b[0;36m_max_pool1d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    709\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[0;32m--> 710\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# MAIN EPOCH LOOP: the epochs are the number of times we loop through the entire training set.\n",
    "for epoch in tnrange(10, desc=\"Total epochs: \"):\n",
    "    \n",
    "    loss_sum = 0\n",
    "    acc_sum = 0\n",
    "    \n",
    "    # BATCH LOOP: loop over the data batches using the data loader \n",
    "    # if you don't have tqdm installed, just use this simpler for-loop instead\n",
    "    # for batch in train_dataloader: \n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # FORWARD PASS and loss calculation\n",
    "        outputs = CNN_model(inputs)\n",
    "        loss = CNN_model.binary_entropy_loss(outputs.sigmoid(), labels)\n",
    "        \n",
    "        # BACKWARD PASS but zero the gradients first to delete the old ones\n",
    "        # as pytorch accumulates gradients by default\n",
    "        CNN_model.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # UPDATE: the model weights are updated\n",
    "        CNN_model.optimizer.step()\n",
    "        \n",
    "        # MONITORING: save loss and accuracy on the batch to track the training\n",
    "        loss_sum += loss.item()\n",
    "\n",
    "        # Get the prediction of the net on the images\n",
    "        predicted = CNN_model.predict(inputs)\n",
    "        acc_sum += (labels == predicted).sum().item() / labels.shape[0]\n",
    "        \n",
    "    \n",
    "    # Print summary of training metrics\n",
    "    loss_avg = loss_sum / len(train_dataloader)\n",
    "    acc_avg = acc_sum / len(train_dataloader)\n",
    "    test_acc = get_accuracy(CNN_model, test_dataloader)\n",
    "    \n",
    "    print(f\"Avg loss: {np.round(loss_avg, 4)} | \"\\\n",
    "          f\"Avg training accuracy: {np.round(acc_avg*100, 2)} | \"\\\n",
    "          f\"Avg validation accuracy: {np.round(test_acc, 2)}\")\n",
    "\n",
    "print('Finished Training') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4fed21f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3b79ef586f24434823385ebee88b6fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Total epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Epoch: 9\n",
      "Optimization ended successfully\n"
     ]
    }
   ],
   "source": [
    "# Now run for 100 epochs\n",
    "for epoch in tnrange(10, desc=\"Total epochs: \"):\n",
    "  \n",
    "    # Clear gradients (pytorch accumulates gradients by default)\n",
    "    CNN_model.optimizer.zero_grad() \n",
    "\n",
    "    # Calculate outputs\n",
    "    output_values = CNN_model(X_train.unsqueeze(1))\n",
    "\n",
    "    # Calculate loss\n",
    "    loss = CNN_model.binary_entropy_loss(output_values.sigmoid(), y_train.unsqueeze(1)) \n",
    "\n",
    "    # Backpropagation & weight adjustment\n",
    "    loss.backward()\n",
    "    CNN_model.optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch: {epoch}\")\n",
    "\n",
    "print(f\"Optimization ended successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "9071fa35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score on test set: 55.13%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Make predictions\n",
    "binary_classification = CNN_model.predict(X_test.unsqueeze(1))\n",
    "\n",
    "# Calculate the score on the test set\n",
    "accuracy = accuracy_score(y_test, binary_classification)\n",
    "print(f\"Accuracy score on test set: {round(accuracy * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a589c4d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecgcap",
   "language": "python",
   "name": "ecgcap"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
